root: INFO: Namespace(arch='alexnet', batch_size=64, bits='48', epochs=3, gamma=200, gpu='0', lamda=2, learning_rate=0.001, max_iter=121, mu=1, num_samples=3000)
root: INFO: 48
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/121][Train Loss: 13.1489]
root: INFO: [Iteration:   1/121][Train Loss: 13.1474]
root: INFO: [Iteration:   2/121][Train Loss: 13.1696]
root: INFO: [Iteration:   3/121][Train Loss: 13.1213]
root: INFO: [Iteration:   4/121][Train Loss: 13.1008]
root: INFO: [Iteration:   5/121][Train Loss: 13.0011]
root: INFO: [Iteration:   6/121][Train Loss: 12.8635]
root: INFO: [Iteration:   7/121][Train Loss: 12.6963]
root: INFO: [Iteration:   8/121][Train Loss: 12.6477]
root: INFO: [Iteration:   9/121][Train Loss: 12.4595]
root: INFO: [Evaluation: mAP: 0.3143, top-5000 mAP: 0.5385]
root: INFO: [Iteration:  10/121][Train Loss: 12.2639]
root: INFO: [Iteration:  11/121][Train Loss: 12.0985]
root: INFO: [Iteration:  12/121][Train Loss: 11.9438]
root: INFO: [Iteration:  13/121][Train Loss: 11.8827]
root: INFO: [Iteration:  14/121][Train Loss: 11.6781]
root: INFO: [Iteration:  15/121][Train Loss: 11.6195]
root: INFO: [Iteration:  16/121][Train Loss: 11.4377]
root: INFO: [Iteration:  17/121][Train Loss: 11.3122]
root: INFO: [Iteration:  18/121][Train Loss: 11.1862]
root: INFO: [Iteration:  19/121][Train Loss: 11.0310]
root: INFO: [Evaluation: mAP: 0.4963, top-5000 mAP: 0.6407]
root: INFO: [Iteration:  20/121][Train Loss: 10.8451]
root: INFO: [Iteration:  21/121][Train Loss: 10.8659]
root: INFO: [Iteration:  22/121][Train Loss: 10.6547]
root: INFO: [Iteration:  23/121][Train Loss: 10.5945]
root: INFO: [Iteration:  24/121][Train Loss: 10.2942]
root: INFO: [Iteration:  25/121][Train Loss: 10.1767]
root: INFO: [Iteration:  26/121][Train Loss: 10.0914]
root: INFO: [Iteration:  27/121][Train Loss: 9.9228]
root: INFO: [Iteration:  28/121][Train Loss: 9.7761]
root: INFO: [Iteration:  29/121][Train Loss: 9.8564]
root: INFO: [Evaluation: mAP: 0.6353, top-5000 mAP: 0.7178]
root: INFO: [Iteration:  30/121][Train Loss: 9.6148]
root: INFO: [Iteration:  31/121][Train Loss: 9.4527]
root: INFO: [Iteration:  32/121][Train Loss: 9.3528]
root: INFO: [Iteration:  33/121][Train Loss: 9.0797]
root: INFO: [Iteration:  34/121][Train Loss: 9.1931]
root: INFO: [Iteration:  35/121][Train Loss: 8.9740]
root: INFO: [Iteration:  36/121][Train Loss: 8.9087]
root: INFO: [Iteration:  37/121][Train Loss: 8.8895]
root: INFO: [Iteration:  38/121][Train Loss: 8.8311]
root: INFO: [Iteration:  39/121][Train Loss: 8.7610]
root: INFO: [Evaluation: mAP: 0.7351, top-5000 mAP: 0.7758]
root: INFO: [Iteration:  40/121][Train Loss: 8.5551]
root: INFO: [Iteration:  41/121][Train Loss: 8.6307]
root: INFO: [Iteration:  42/121][Train Loss: 8.4780]
root: INFO: [Iteration:  43/121][Train Loss: 8.4919]
root: INFO: [Iteration:  44/121][Train Loss: 7.9457]
root: INFO: [Iteration:  45/121][Train Loss: 8.1294]
root: INFO: [Iteration:  46/121][Train Loss: 8.0076]
root: INFO: [Iteration:  47/121][Train Loss: 7.9803]
root: INFO: [Iteration:  48/121][Train Loss: 8.0030]
root: INFO: [Iteration:  49/121][Train Loss: 7.8655]
root: INFO: [Evaluation: mAP: 0.8002, top-5000 mAP: 0.8174]
root: INFO: [Iteration:  50/121][Train Loss: 7.5968]
root: INFO: [Iteration:  51/121][Train Loss: 7.7195]
root: INFO: [Iteration:  52/121][Train Loss: 7.5738]
root: INFO: [Iteration:  53/121][Train Loss: 7.2567]
root: INFO: [Iteration:  54/121][Train Loss: 7.5574]
root: INFO: [Iteration:  55/121][Train Loss: 7.3655]
root: INFO: [Iteration:  56/121][Train Loss: 7.3384]
root: INFO: [Iteration:  57/121][Train Loss: 7.1875]
root: INFO: [Iteration:  58/121][Train Loss: 7.2245]
root: INFO: [Iteration:  59/121][Train Loss: 7.2812]
root: INFO: [Evaluation: mAP: 0.8336, top-5000 mAP: 0.8329]
root: INFO: [Iteration:  60/121][Train Loss: 7.1705]
root: INFO: [Iteration:  61/121][Train Loss: 6.8955]
root: INFO: [Iteration:  62/121][Train Loss: 6.8291]
root: INFO: [Iteration:  63/121][Train Loss: 6.7216]
root: INFO: [Iteration:  64/121][Train Loss: 6.5921]
root: INFO: [Iteration:  65/121][Train Loss: 6.5188]
root: INFO: [Iteration:  66/121][Train Loss: 6.5584]
root: INFO: [Iteration:  67/121][Train Loss: 6.7907]
root: INFO: [Iteration:  68/121][Train Loss: 6.4786]
root: INFO: [Iteration:  69/121][Train Loss: 6.5028]
root: INFO: [Evaluation: mAP: 0.8522, top-5000 mAP: 0.8366]
root: INFO: [Iteration:  70/121][Train Loss: 6.3965]
root: INFO: [Iteration:  71/121][Train Loss: 6.3863]
root: INFO: [Iteration:  72/121][Train Loss: 6.2248]
root: INFO: [Iteration:  73/121][Train Loss: 6.3707]
root: INFO: [Iteration:  74/121][Train Loss: 6.3239]
root: INFO: [Iteration:  75/121][Train Loss: 6.2498]
root: INFO: [Iteration:  76/121][Train Loss: 6.1956]
root: INFO: [Iteration:  77/121][Train Loss: 5.9858]
root: INFO: [Iteration:  78/121][Train Loss: 6.1406]
root: INFO: [Iteration:  79/121][Train Loss: 6.0947]
root: INFO: [Evaluation: mAP: 0.8735, top-5000 mAP: 0.8557]
root: INFO: [Iteration:  80/121][Train Loss: 5.8664]
root: INFO: [Iteration:  81/121][Train Loss: 5.7883]
root: INFO: [Iteration:  82/121][Train Loss: 6.0249]
root: INFO: [Iteration:  83/121][Train Loss: 5.9547]
root: INFO: [Iteration:  84/121][Train Loss: 6.1291]
root: INFO: [Iteration:  85/121][Train Loss: 5.8123]
root: INFO: [Iteration:  86/121][Train Loss: 5.7973]
root: INFO: [Iteration:  87/121][Train Loss: 5.8440]
root: INFO: [Iteration:  88/121][Train Loss: 5.5442]
root: INFO: [Iteration:  89/121][Train Loss: 5.6458]
root: INFO: [Evaluation: mAP: 0.8786, top-5000 mAP: 0.8552]
root: INFO: [Iteration:  90/121][Train Loss: 5.6817]
root: INFO: [Iteration:  91/121][Train Loss: 5.6049]
root: INFO: [Iteration:  92/121][Train Loss: 5.4638]
root: INFO: [Iteration:  93/121][Train Loss: 5.4119]
root: INFO: [Iteration:  94/121][Train Loss: 5.3130]
root: INFO: [Iteration:  95/121][Train Loss: 5.3992]
root: INFO: [Iteration:  96/121][Train Loss: 5.4233]
root: INFO: [Iteration:  97/121][Train Loss: 5.4703]
root: INFO: [Iteration:  98/121][Train Loss: 5.3933]
root: INFO: [Iteration:  99/121][Train Loss: 5.2747]
root: INFO: [Evaluation: mAP: 0.8831, top-5000 mAP: 0.8573]
root: INFO: [Iteration: 100/121][Train Loss: 5.3286]
root: INFO: [Iteration: 101/121][Train Loss: 5.2980]
root: INFO: [Iteration: 102/121][Train Loss: 5.3645]
root: INFO: [Iteration: 103/121][Train Loss: 5.4990]
root: INFO: [Iteration: 104/121][Train Loss: 5.3819]
root: INFO: [Iteration: 105/121][Train Loss: 5.1549]
root: INFO: [Iteration: 106/121][Train Loss: 5.1092]
root: INFO: [Iteration: 107/121][Train Loss: 5.1635]
root: INFO: [Iteration: 108/121][Train Loss: 5.1336]
root: INFO: [Iteration: 109/121][Train Loss: 5.0553]
root: INFO: [Evaluation: mAP: 0.8853, top-5000 mAP: 0.8576]
root: INFO: [Iteration: 110/121][Train Loss: 4.9609]
root: INFO: [Iteration: 111/121][Train Loss: 4.9763]
root: INFO: [Iteration: 112/121][Train Loss: 4.9567]
root: INFO: [Iteration: 113/121][Train Loss: 4.9746]
root: INFO: [Iteration: 114/121][Train Loss: 4.8343]
root: INFO: [Iteration: 115/121][Train Loss: 4.9205]
root: INFO: [Iteration: 116/121][Train Loss: 4.9824]
root: INFO: [Iteration: 117/121][Train Loss: 4.6986]
root: INFO: [Iteration: 118/121][Train Loss: 4.8145]
root: INFO: [Iteration: 119/121][Train Loss: 5.1012]
root: INFO: [Evaluation: mAP: 0.8946, top-5000 mAP: 0.8681]
root: INFO: [Iteration: 120/121][Train Loss: 4.8295]
