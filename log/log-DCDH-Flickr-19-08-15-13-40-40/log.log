root: INFO: Namespace(arch='alexnet', batch_size=64, bits='48', epochs=3, gamma=200, gpu='1', lamda=0.5, learning_rate=0.001, max_iter=121, mu=0.5, num_samples=3000)
root: INFO: 48
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/121][Train Loss: 13.1229]
root: INFO: [Iteration:   1/121][Train Loss: 13.1071]
root: INFO: [Iteration:   2/121][Train Loss: 13.1371]
root: INFO: [Iteration:   3/121][Train Loss: 13.1013]
root: INFO: [Iteration:   4/121][Train Loss: 13.0746]
root: INFO: [Iteration:   5/121][Train Loss: 13.0262]
root: INFO: [Iteration:   6/121][Train Loss: 12.9543]
root: INFO: [Iteration:   7/121][Train Loss: 12.8734]
root: INFO: [Iteration:   8/121][Train Loss: 12.8023]
root: INFO: [Iteration:   9/121][Train Loss: 12.6623]
root: INFO: [Evaluation: mAP: 0.3155, top-5000 mAP: 0.5359]
root: INFO: [Iteration:  10/121][Train Loss: 12.4395]
root: INFO: [Iteration:  11/121][Train Loss: 12.3825]
root: INFO: [Iteration:  12/121][Train Loss: 12.2735]
root: INFO: [Iteration:  13/121][Train Loss: 12.1089]
root: INFO: [Iteration:  14/121][Train Loss: 12.0450]
root: INFO: [Iteration:  15/121][Train Loss: 11.9067]
root: INFO: [Iteration:  16/121][Train Loss: 11.8912]
root: INFO: [Iteration:  17/121][Train Loss: 11.6934]
root: INFO: [Iteration:  18/121][Train Loss: 11.5983]
root: INFO: [Iteration:  19/121][Train Loss: 11.4394]
root: INFO: [Evaluation: mAP: 0.4847, top-5000 mAP: 0.6189]
root: INFO: [Iteration:  20/121][Train Loss: 11.3645]
root: INFO: [Iteration:  21/121][Train Loss: 11.4310]
root: INFO: [Iteration:  22/121][Train Loss: 11.1136]
root: INFO: [Iteration:  23/121][Train Loss: 11.0865]
root: INFO: [Iteration:  24/121][Train Loss: 11.0104]
root: INFO: [Iteration:  25/121][Train Loss: 10.8707]
root: INFO: [Iteration:  26/121][Train Loss: 10.7117]
root: INFO: [Iteration:  27/121][Train Loss: 10.7803]
root: INFO: [Iteration:  28/121][Train Loss: 10.4925]
root: INFO: [Iteration:  29/121][Train Loss: 10.5194]
root: INFO: [Evaluation: mAP: 0.6081, top-5000 mAP: 0.6764]
root: INFO: [Iteration:  30/121][Train Loss: 10.3276]
root: INFO: [Iteration:  31/121][Train Loss: 10.2350]
root: INFO: [Iteration:  32/121][Train Loss: 10.2037]
root: INFO: [Iteration:  33/121][Train Loss: 10.2033]
root: INFO: [Iteration:  34/121][Train Loss: 10.1382]
root: INFO: [Iteration:  35/121][Train Loss: 10.0417]
root: INFO: [Iteration:  36/121][Train Loss: 9.9121]
root: INFO: [Iteration:  37/121][Train Loss: 9.7004]
root: INFO: [Iteration:  38/121][Train Loss: 9.7912]
root: INFO: [Iteration:  39/121][Train Loss: 9.8030]
root: INFO: [Evaluation: mAP: 0.6876, top-5000 mAP: 0.7172]
root: INFO: [Iteration:  40/121][Train Loss: 9.6688]
root: INFO: [Iteration:  41/121][Train Loss: 9.5435]
root: INFO: [Iteration:  42/121][Train Loss: 9.4122]
root: INFO: [Iteration:  43/121][Train Loss: 9.2907]
root: INFO: [Iteration:  44/121][Train Loss: 9.3172]
root: INFO: [Iteration:  45/121][Train Loss: 9.4584]
root: INFO: [Iteration:  46/121][Train Loss: 9.2190]
root: INFO: [Iteration:  47/121][Train Loss: 9.2867]
root: INFO: [Iteration:  48/121][Train Loss: 9.2155]
root: INFO: [Iteration:  49/121][Train Loss: 8.9040]
root: INFO: [Evaluation: mAP: 0.7562, top-5000 mAP: 0.7666]
root: INFO: [Iteration:  50/121][Train Loss: 8.8941]
root: INFO: [Iteration:  51/121][Train Loss: 8.7761]
root: INFO: [Iteration:  52/121][Train Loss: 8.6451]
root: INFO: [Iteration:  53/121][Train Loss: 8.5080]
root: INFO: [Iteration:  54/121][Train Loss: 8.5397]
root: INFO: [Iteration:  55/121][Train Loss: 8.6792]
root: INFO: [Iteration:  56/121][Train Loss: 8.3328]
root: INFO: [Iteration:  57/121][Train Loss: 8.5971]
root: INFO: [Iteration:  58/121][Train Loss: 8.3222]
root: INFO: [Iteration:  59/121][Train Loss: 8.2759]
root: INFO: [Evaluation: mAP: 0.7959, top-5000 mAP: 0.7874]
root: INFO: [Iteration:  60/121][Train Loss: 8.2763]
root: INFO: [Iteration:  61/121][Train Loss: 8.2227]
root: INFO: [Iteration:  62/121][Train Loss: 8.0863]
root: INFO: [Iteration:  63/121][Train Loss: 8.0261]
root: INFO: [Iteration:  64/121][Train Loss: 8.1434]
root: INFO: [Iteration:  65/121][Train Loss: 8.1108]
root: INFO: [Iteration:  66/121][Train Loss: 7.8277]
root: INFO: [Iteration:  67/121][Train Loss: 7.9203]
root: INFO: [Iteration:  68/121][Train Loss: 7.7835]
root: INFO: [Iteration:  69/121][Train Loss: 7.5389]
root: INFO: [Evaluation: mAP: 0.8217, top-5000 mAP: 0.8055]
root: INFO: [Iteration:  70/121][Train Loss: 7.6149]
root: INFO: [Iteration:  71/121][Train Loss: 7.6805]
root: INFO: [Iteration:  72/121][Train Loss: 7.7503]
root: INFO: [Iteration:  73/121][Train Loss: 7.5801]
root: INFO: [Iteration:  74/121][Train Loss: 7.5407]
root: INFO: [Iteration:  75/121][Train Loss: 7.3218]
root: INFO: [Iteration:  76/121][Train Loss: 7.4955]
root: INFO: [Iteration:  77/121][Train Loss: 7.1923]
root: INFO: [Iteration:  78/121][Train Loss: 7.3204]
root: INFO: [Iteration:  79/121][Train Loss: 7.2688]
root: INFO: [Evaluation: mAP: 0.8428, top-5000 mAP: 0.8219]
root: INFO: [Iteration:  80/121][Train Loss: 7.3477]
root: INFO: [Iteration:  81/121][Train Loss: 7.3505]
root: INFO: [Iteration:  82/121][Train Loss: 7.0130]
root: INFO: [Iteration:  83/121][Train Loss: 7.2877]
root: INFO: [Iteration:  84/121][Train Loss: 7.1634]
root: INFO: [Iteration:  85/121][Train Loss: 6.9148]
root: INFO: [Iteration:  86/121][Train Loss: 7.0177]
root: INFO: [Iteration:  87/121][Train Loss: 6.9493]
root: INFO: [Iteration:  88/121][Train Loss: 6.8852]
root: INFO: [Iteration:  89/121][Train Loss: 6.7795]
root: INFO: [Evaluation: mAP: 0.8574, top-5000 mAP: 0.8344]
root: INFO: [Iteration:  90/121][Train Loss: 6.5854]
root: INFO: [Iteration:  91/121][Train Loss: 6.8010]
root: INFO: [Iteration:  92/121][Train Loss: 7.0533]
root: INFO: [Iteration:  93/121][Train Loss: 6.9887]
root: INFO: [Iteration:  94/121][Train Loss: 6.6797]
root: INFO: [Iteration:  95/121][Train Loss: 6.8029]
root: INFO: [Iteration:  96/121][Train Loss: 6.7095]
root: INFO: [Iteration:  97/121][Train Loss: 6.7594]
root: INFO: [Iteration:  98/121][Train Loss: 6.6853]
root: INFO: [Iteration:  99/121][Train Loss: 6.6386]
root: INFO: [Evaluation: mAP: 0.8622, top-5000 mAP: 0.8376]
root: INFO: [Iteration: 100/121][Train Loss: 6.7032]
root: INFO: [Iteration: 101/121][Train Loss: 6.5261]
root: INFO: [Iteration: 102/121][Train Loss: 6.3085]
root: INFO: [Iteration: 103/121][Train Loss: 6.2712]
root: INFO: [Iteration: 104/121][Train Loss: 6.4742]
root: INFO: [Iteration: 105/121][Train Loss: 6.4255]
root: INFO: [Iteration: 106/121][Train Loss: 6.7877]
root: INFO: [Iteration: 107/121][Train Loss: 6.6126]
root: INFO: [Iteration: 108/121][Train Loss: 6.3080]
root: INFO: [Iteration: 109/121][Train Loss: 6.4300]
root: INFO: [Evaluation: mAP: 0.8670, top-5000 mAP: 0.8389]
root: INFO: [Iteration: 110/121][Train Loss: 6.4439]
root: INFO: [Iteration: 111/121][Train Loss: 6.0920]
root: INFO: [Iteration: 112/121][Train Loss: 6.3854]
root: INFO: [Iteration: 113/121][Train Loss: 6.1410]
root: INFO: [Iteration: 114/121][Train Loss: 5.9283]
root: INFO: [Iteration: 115/121][Train Loss: 6.0745]
root: INFO: [Iteration: 116/121][Train Loss: 6.2044]
root: INFO: [Iteration: 117/121][Train Loss: 6.0663]
root: INFO: [Iteration: 118/121][Train Loss: 6.1032]
root: INFO: [Iteration: 119/121][Train Loss: 6.0877]
root: INFO: [Evaluation: mAP: 0.8738, top-5000 mAP: 0.8455]
root: INFO: [Iteration: 120/121][Train Loss: 6.0849]
