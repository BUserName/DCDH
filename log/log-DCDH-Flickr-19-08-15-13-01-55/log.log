root: INFO: Namespace(arch='alexnet', batch_size=64, bits='48', epochs=3, gamma=200, gpu='1', lamda=0.5, learning_rate=0.001, max_iter=121, mu=0.5, num_samples=3000)
root: INFO: 48
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/121][Train Loss: 347.7088]
root: INFO: [Iteration:   1/121][Train Loss: 339.1214]
root: INFO: [Iteration:   2/121][Train Loss: 315.6890]
root: INFO: [Iteration:   3/121][Train Loss: 325.0431]
root: INFO: [Iteration:   4/121][Train Loss: 292.5805]
root: INFO: [Iteration:   5/121][Train Loss: 297.7962]
root: INFO: [Iteration:   6/121][Train Loss: 296.8324]
root: INFO: [Iteration:   7/121][Train Loss: 288.4512]
root: INFO: [Iteration:   8/121][Train Loss: 297.5534]
root: INFO: [Iteration:   9/121][Train Loss: 285.4633]
root: INFO: [Evaluation: mAP: 0.8031, top-5000 mAP: 0.8830]
root: INFO: [Iteration:  10/121][Train Loss: 287.0840]
root: INFO: [Iteration:  11/121][Train Loss: 276.9147]
root: INFO: [Iteration:  12/121][Train Loss: 272.9063]
root: INFO: [Iteration:  13/121][Train Loss: 278.7441]
root: INFO: [Iteration:  14/121][Train Loss: 279.6269]
root: INFO: [Iteration:  15/121][Train Loss: 267.9503]
root: INFO: [Iteration:  16/121][Train Loss: 272.6519]
root: INFO: [Iteration:  17/121][Train Loss: 247.9531]
root: INFO: [Iteration:  18/121][Train Loss: 251.9733]
root: INFO: [Iteration:  19/121][Train Loss: 247.2778]
root: INFO: [Evaluation: mAP: 0.8442, top-5000 mAP: 0.8974]
root: INFO: [Iteration:  20/121][Train Loss: 257.0586]
root: INFO: [Iteration:  21/121][Train Loss: 265.2064]
root: INFO: [Iteration:  22/121][Train Loss: 259.3956]
root: INFO: [Iteration:  23/121][Train Loss: 261.5137]
root: INFO: [Iteration:  24/121][Train Loss: 278.9703]
root: INFO: [Iteration:  25/121][Train Loss: 256.8683]
root: INFO: [Iteration:  26/121][Train Loss: 248.8894]
root: INFO: [Iteration:  27/121][Train Loss: 268.8542]
root: INFO: [Iteration:  28/121][Train Loss: 235.3638]
root: INFO: [Iteration:  29/121][Train Loss: 256.7735]
root: INFO: [Evaluation: mAP: 0.8519, top-5000 mAP: 0.8984]
root: INFO: [Iteration:  30/121][Train Loss: 252.1932]
root: INFO: [Iteration:  31/121][Train Loss: 258.6641]
root: INFO: [Iteration:  32/121][Train Loss: 262.2727]
root: INFO: [Iteration:  33/121][Train Loss: 246.8283]
root: INFO: [Iteration:  34/121][Train Loss: 231.5758]
root: INFO: [Iteration:  35/121][Train Loss: 251.9663]
root: INFO: [Iteration:  36/121][Train Loss: 261.8682]
root: INFO: [Iteration:  37/121][Train Loss: 241.4401]
root: INFO: [Iteration:  38/121][Train Loss: 240.4551]
